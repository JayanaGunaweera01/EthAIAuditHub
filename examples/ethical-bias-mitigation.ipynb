{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\nfrom aif360.algorithms.preprocessing import Reweighing, DisparateImpactRemover\nfrom aif360.metrics import BinaryLabelDatasetMetric\nfrom aif360.datasets import BinaryLabelDataset\nimport aif360.datasets.german_credit_dataset as gcr\nfrom aif360.datasets import StandardDataset\nimport alibi.explainers # ALIBI implementation\nimport gif # Google Impact Framework implementation\n\n# Load the dataset\ndata_orig = gcr.GermanCreditDataset(protected_attribute_names=['Gender', 'Race'],\n         privileged_classes=[['Male'], ['White']])\ndata_orig_train, data_orig_test = data_orig.split([0.7], shuffle=True)\n\n# Impute missing values\nimputer = SimpleImputer(strategy='mean')\ndata_orig_train.features = imputer.fit_transform(data_orig_train.features)\ndata_orig_test.features = imputer.transform(data_orig_test.features)\n\n# Train fairness-aware model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(data_orig_train.features, data_orig_train.labels.ravel())\n\n# Statistical analysis with AIF360\nprivileged_groups = [{'Gender': 1, 'Race': 1}] # Assuming 1 represents privileged\nunprivileged_groups = [{'Gender': 0, 'Race': 0}] # Assuming 0 represents unprivileged\n\n# Create BinaryLabelDataset for AIF360 metrics\ndata_pred = data_orig_test.copy()\ndata_pred.labels = model.predict(data_orig_test.features)\n\nmetric_orig_train = BinaryLabelDatasetMetric(data_orig_train, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\nmetric_orig_test = BinaryLabelDatasetMetric(data_orig_test, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\nmetric_pred = BinaryLabelDatasetMetric(data_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n\nprint(\"Original training set: Disparate Impact =\", metric_orig_train.disparate_impact())\nprint(\"Original test set: Disparate Impact =\", metric_orig_test.disparate_impact())\nprint(\"Predicted test set: Disparate Impact =\", metric_pred.disparate_impact())\n\n# Bias mitigation with AIF360\nrw = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\ndata_reweighted_train = rw.fit_transform(data_orig_train)\n\ndi_removal = DisparateImpactRemover(repair_level=1.0)\ndata_repaired_train = di_removal.fit_transform(data_orig_train)\n\nmodel_reweighted = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel_repaired = RandomForestClassifier(n_estimators=100, random_state=42)\n\nmodel_reweighted.fit(data_reweighted_train.features, data_reweighted_train.labels.ravel())\nmodel_repaired.fit(data_repaired_train.features, data_repaired_train.labels.ravel())\n\n\n\n# Use Google Impact Framework for broader ethical analysis\nethical_analyzer = gif.EthicalAnalyzer()\ncontextual_insights = ethical_analyzer.analyze(data_orig_test.features, data_orig_test.labels)\n\n# Comprehensive ethical analysis with AIF360\nprint(\"Group Fairness Metrics:\")\nprint(\"----------------------------------\")\nprint(\"Statistical Parity Difference:\", metric_orig_test.statistical_parity_difference())\nprint(\"Equal Opportunity Difference:\", metric_orig_test.equal_opportunity_difference())\nprint(\"Average Odds Difference:\", metric_orig_test.average_odds_difference())\nprint(\"Theil Index:\", ClassificationMetric(metric_orig_test, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups).theil_index())\nprint(\"----------------------------------\")\n\n# Subgroup Analysis for Intersectional Biases\nsubgroup_metric = metric_orig_test.get_metric_by_group(unprivileged_groups=[{'Gender': 0, 'Race': 0}], privileged_groups=[{'Gender': 1, 'Race': 1}])\nprint(\"Subgroup Analysis (Gender=0, Race=0):\")\nprint(\"Statistical Parity Difference:\", subgroup_metric.statistical_parity_difference())\nprint(\"Equal Opportunity Difference:\", subgroup_metric.equal_opportunity_difference())\nprint(\"Average Odds Difference:\", subgroup_metric.average_odds_difference())\nprint(\"----------------------------------\")\n\n\n# Expand to include more diverse subgroups\nfor gender in [0, 1]:\n    for race in [0, 1]:\n        subgroup_metric = metric_orig_test.get_metric_by_group(\n            unprivileged_groups=[{'Gender': gender, 'Race': race}],\n            privileged_groups=[{'Gender': 1 - gender, 'Race': 1 - race}]\n        )\n        print(f\"Subgroup Analysis (Gender={gender}, Race={race}):\")\n        print(\"Statistical Parity Difference:\", subgroup_metric.statistical_parity_difference())\n        print(\"Equal Opportunity Difference:\", subgroup_metric.equal_opportunity_difference())\n        print(\"Average Odds Difference:\", subgroup_metric.average_odds_difference())\n        print(\"----------------------------------\")\n\n\n# Use ALIBI for model explainability\nexplainer = alibi.explainers.TreeExplainer(model_repaired)\nshap_values = explainer.shap_values(data_orig_test.features)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}